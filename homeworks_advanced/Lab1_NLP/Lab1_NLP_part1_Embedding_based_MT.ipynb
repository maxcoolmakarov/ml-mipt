{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eulvfJWl7ueY"
   },
   "source": [
    "# Lab 1\n",
    "\n",
    "\n",
    "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT (30%)\n",
    "*Note: this homework is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
    "\n",
    "*Refined by [Nikolay Karpachev](https://www.linkedin.com/in/nikolay-karpachev-b0146a104/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fV4rIjxa7uei"
   },
   "source": [
    "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
    "\n",
    "But even without parallel corpora this system can be good enough (hopefully), in particular for similar languages, e.g. Ukrainian and Russian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idSYq2GU7uew"
   },
   "source": [
    "### Frament of the Swadesh list for some slavic languages\n",
    "\n",
    "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
    "\n",
    "So we can see some kind of word invariance for different Slavic languages.\n",
    "\n",
    "\n",
    "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
    "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
    "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
    "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
    "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
    "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
    "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
    "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
    "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
    "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
    "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
    "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
    "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
    "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
    "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
    "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
    "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
    "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
    "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
    "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
    "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
    "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
    "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNM3_fjr7ue2"
   },
   "source": [
    "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLppwa527ue6"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYBGKAUn7ue_"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwGoVhRA7ufP"
   },
   "source": [
    "In this notebook we're going to use pretrained word vectors - FastText (original paper - https://arxiv.org/abs/1607.04606).\n",
    "\n",
    "You can download them from the official [website](https://fasttext.cc/docs/en/crawl-vectors.html). We're going to need embeddings for Russian and Ukrainian languages. Please use word2vec-compatible format (.text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1JjQv_97ufT"
   },
   "outputs": [],
   "source": [
    "uk_emb = KeyedVectors.load_word2vec_format(\"C:/Users/maxim/Downloads/cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffzuept_7ufd"
   },
   "outputs": [],
   "source": [
    "ru_emb = KeyedVectors.load_word2vec_format('C:/Users/maxim/Downloads/cc.ru.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTkXfT0W7ufk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('август', 1.0),\n",
       " ('июль', 0.9383153319358826),\n",
       " ('сентябрь', 0.9240028858184814),\n",
       " ('июнь', 0.9222576022148132),\n",
       " ('октябрь', 0.9095539450645447),\n",
       " ('ноябрь', 0.8930035829544067),\n",
       " ('апрель', 0.8729087114334106),\n",
       " ('декабрь', 0.8652557730674744),\n",
       " ('март', 0.8545796871185303),\n",
       " ('февраль', 0.8401416540145874)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdBA8lcg7ufs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('серпень', 1.0),\n",
       " ('липень', 0.9096439480781555),\n",
       " ('вересень', 0.9016969203948975),\n",
       " ('червень', 0.8992519974708557),\n",
       " ('жовтень', 0.8810408115386963),\n",
       " ('листопад', 0.8787633776664734),\n",
       " ('квітень', 0.8592804670333862),\n",
       " ('грудень', 0.8586863279342651),\n",
       " ('травень', 0.8408110737800598),\n",
       " ('лютий', 0.8256431221961975)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yJvcKXO7uf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Stepashka.com', 0.2757962942123413),\n",
       " ('ЖИЗНИВадим', 0.25203439593315125),\n",
       " ('2Дмитрий', 0.25048112869262695),\n",
       " ('2012Дмитрий', 0.24829229712486267),\n",
       " ('Ведущий-Алексей', 0.2443869709968567),\n",
       " ('Недопустимость', 0.24435287714004517),\n",
       " ('2Михаил', 0.23981398344039917),\n",
       " ('лексей', 0.23740758001804352),\n",
       " ('комплексн', 0.23695147037506104),\n",
       " ('персональ', 0.2368222177028656)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNdYAR1q7uf6"
   },
   "source": [
    "Load small dictionaries for correspoinding words pairs as trainset and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35d_DAK67uf8"
   },
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkNL602WHJyO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 59351\r",
      " 13% [..........                                                                    ]  8192 / 59351\r",
      " 27% [.....................                                                         ] 16384 / 59351\r",
      " 41% [................................                                              ] 24576 / 59351\r",
      " 55% [...........................................                                   ] 32768 / 59351\r",
      " 69% [.....................................................                         ] 40960 / 59351\r",
      " 82% [................................................................              ] 49152 / 59351\r",
      " 96% [...........................................................................   ] 57344 / 59351\r",
      "100% [..............................................................................] 59351 / 59351"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ukr_rus.train.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget \n",
    "wget.download('http://tiny.cc/jfgecz', 'ukr_rus.train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoclU6JcHCcn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 12188\r",
      " 67% [....................................................                          ]  8192 / 12188\r",
      "100% [..............................................................................] 12188 / 12188"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ukr_rus.test.txt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget -O ukr_rus.test.txt http://tiny.cc/6zoeez\n",
    "wget.download('http://tiny.cc/6zoeez', 'ukr_rus.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05BqsdSK7ugD"
   },
   "outputs": [],
   "source": [
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQOZw51r7ugL"
   },
   "outputs": [],
   "source": [
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZBBNvpz7ugQ"
   },
   "source": [
    "## Embedding space mapping (0.3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_Dhk5gL7ugS"
   },
   "source": [
    "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$$\n",
    "or\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F$$\n",
    "\n",
    "where $||*||_F$ - Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acOjDdtL7ugY"
   },
   "source": [
    "$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$ looks like simple multiple linear regression (without intercept fit). So let's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb-KN1be7uga"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mapping = LinearRegression()\n",
    "mapping.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7tqJwoY7ugf"
   },
   "source": [
    "Let's take a look at neigbours of the vector of word _\"серпень\"_ (_\"август\"_ in Russian) after linear transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31SrFSbn7ugi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('июнь', 0.8402308225631714)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.8531404137611389),\n",
       " ('июнь', 0.8402308225631714),\n",
       " ('март', 0.8385775089263916),\n",
       " ('сентябрь', 0.8331868648529053),\n",
       " ('февраль', 0.8311494588851929),\n",
       " ('октябрь', 0.8278172016143799),\n",
       " ('ноябрь', 0.8244151473045349),\n",
       " ('июль', 0.822899580001831),\n",
       " ('август', 0.8112362623214722),\n",
       " ('январь', 0.8022860288619995)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okSkjk597ugo"
   },
   "source": [
    "We can see that neighbourhood of this embedding cosists of different months, but right variant is on the ninth place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2uY6Y9B7ugt"
   },
   "source": [
    "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed Ukrainian embedding we count how many right target pairs are found in top N nearest neighbours in Russian embedding space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zptuho8LAfIE"
   },
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (_, ru) in enumerate(pairs):\n",
    "        # YOUR CODE HERE\n",
    "        if ru in list(map(lambda f: f[0], ru_emb.most_similar([mapped_vectors[i]])[:topn])):\n",
    "            num_matches += 1       \n",
    "        \n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duhj9hpv7ugy"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-iyd5gP7ug5"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-ssEJ3x7uhA"
   },
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K-hy7a6Ksn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6259541984732825\n",
      "0.7913486005089059\n"
     ]
    }
   ],
   "source": [
    "print(precision_top1)\n",
    "print(precision_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hf6Ou8bx7uhH"
   },
   "source": [
    "## Making it better (orthogonal Procrustean problem) (0.3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oLs-drN7uhK"
   },
   "source": [
    "It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
    "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- identity matrix}$$\n",
    "\n",
    "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "$$W^*=UV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KSaRJFGMFiJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdFQ7qti7uhL"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\" \n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "#     mapping = LinearRegression()\n",
    "#     mapping.fit(X_train, Y_train)\n",
    "#     u, sig, v = np.linalg.svd(mapping.coef_)\n",
    "#     mapping = np.matmul(u, v)\n",
    "    u, sig, v = np.linalg.svd(np.matmul(X_train.T, Y_train))\n",
    "    mapping = np.matmul(u, v)\n",
    "    # compute orthogonal embedding space mapping\n",
    "    # mapping = ...\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7X7QfYDd7uhQ"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVOFYYa37uhX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.8245131969451904),\n",
       " ('июнь', 0.8056630492210388),\n",
       " ('сентябрь', 0.8055762052536011),\n",
       " ('март', 0.8032935857772827),\n",
       " ('октябрь', 0.7987102270126343),\n",
       " ('июль', 0.7946797013282776),\n",
       " ('ноябрь', 0.7939636707305908),\n",
       " ('август', 0.7938188910484314),\n",
       " ('февраль', 0.7923861742019653),\n",
       " ('декабрь', 0.7715375423431396)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r297sYP37uhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6437659033078881\n",
      "0.7989821882951654\n"
     ]
    }
   ],
   "source": [
    "print(precision(uk_ru_test, np.matmul(X_test, W)))\n",
    "print(precision(uk_ru_test, np.matmul(X_test, W), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvUZ72U5AfJg"
   },
   "source": [
    "## Unsupervised embedding-based MT (0.4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLyuVfHBLrJn"
   },
   "source": [
    "Now, let's build our word embeddings-based translator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPAURW1CMuP7"
   },
   "source": [
    "Firstly, download OPUS Tatoeba corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F80kUKzQMsDu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 1819128 / 1819128"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'uk.txt.gz'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz\n",
    "wget.download('https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz', 'uk.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CGFZoxCUVf1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"gzip\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!gzip -d ./uk.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MV3VvoVUX5U"
   },
   "outputs": [],
   "source": [
    "with open('./uk.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    uk_corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tU7nPVf0UhbI"
   },
   "outputs": [],
   "source": [
    "# To save your time and CPU, feel free to use first 1000 sentences of the corpus\n",
    "uk_corpus = uk_corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGksC7l_NMi9"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    sentence_emb = [uk_emb[word] if word in uk_emb else 'UNKNOWN' for word in sentence.split(' ')]\n",
    "    sentence_emb = [[np.matmul(word, W)] if word != 'UNKNOWN' else 'UNKNOWN' for word in sentence_emb]\n",
    "    translated = [ru_emb.most_similar(word)[0][0] if word != 'UNKNOWN' else 'UNKNOWN' for word in sentence_emb]\n",
    "\n",
    "    return \" \".join(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hbbMy-tNxlf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-3173ebdff713>:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  sentence_emb = [[np.matmul(word, W)] if word != 'UNKNOWN' else 'UNKNOWN' for word in sentence_emb]\n"
     ]
    }
   ],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ia6I2ce7O_HI"
   },
   "source": [
    "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ap1W7ZCeOAVU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я вже закінчу коледж, коли ви вернетеся з Америки.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-3173ebdff713>:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  sentence_emb = [[np.matmul(word, W)] if word != 'UNKNOWN' else 'UNKNOWN' for word in sentence_emb]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я уже закончу UNKNOWN когда мы прибежишь со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Місто бомбардували ворожі літаки.\n",
      "\n",
      "Город бомбили враждебные UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Можливо, я антисоціальний, але це не означає, що я не спілкуюся з людьми.\n",
      "\n",
      "UNKNOWN мной UNKNOWN конечно это не UNKNOWN что мной не общаюсь со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Цього ранку випала роса.\n",
      "\n",
      "Впрочем утра выпала UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Біда не приходить одна.\n",
      "\n",
      "Беда не приходит UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Подивися на той дим.\n",
      "\n",
      "Посмотри по тот UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я замовив два гамбургера.\n",
      "\n",
      "Я заказал два UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я не хотів нікого образити.\n",
      "\n",
      "Я не хотел никого UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Гора вкрита снігом.\n",
      "\n",
      "Гора покрыта UNKNOWN\n",
      "-------------------------------------------------------\n",
      "На фотографії в дівчини корона не з золота, а з квітів.\n",
      "\n",
      "по фотографии во девушки корона не со UNKNOWN а со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "У мене є мрія.\n",
      "\n",
      "Во меня То UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я приїхав у Японію з Китаю.\n",
      "\n",
      "Я приехал во Японию со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "На півночі знаходиться Шотландія; на півдні — Англія; на заході — Уельс; і ще далі на заході — Північна Ірландія.\n",
      "\n",
      "по север находится UNKNOWN по юге — UNKNOWN по востоке — UNKNOWN и ещe дальше по востоке — северная UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Його рідна країна — Німеччина.\n",
      "\n",
      "Его родная страна — UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Берн — столиця Швейцарії.\n",
      "\n",
      "Берн — столица UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він чекав на нього до десятої години.\n",
      "\n",
      "Он ждал по него к десятой UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ти можеш взяти цю книгу даром.\n",
      "\n",
      "Ты можешь взять ту книгу UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Цей роман написав відомий американський письменник.\n",
      "\n",
      "Такой роман сочинил известный американский UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Забронюйте, будьте ласкаві, кімнату біля міжнародного аеропорту в Торонто.\n",
      "\n",
      "UNKNOWN будте UNKNOWN комнату возле международного аэропорта во UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він знає, що ти його кохаєш?\n",
      "\n",
      "Он UNKNOWN что ты его UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я знаю, що ти багатий.\n",
      "\n",
      "Я UNKNOWN что ты UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ті, хто все забувають, щасливі.\n",
      "\n",
      "UNKNOWN кто всё UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "В цій річці небезпечно плавати.\n",
      "\n",
      "Во этой реке опасно UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Прийшов, побачив, переміг.\n",
      "\n",
      "UNKNOWN UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я ходжу до школи пішки.\n",
      "\n",
      "Я хожу к школы UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Не твоя справа!\n",
      "\n",
      "Не моя UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Не забудь квиток.\n",
      "\n",
      "Не забудь UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Хто він?\n",
      "\n",
      "Кто UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ви будете чай чи каву?\n",
      "\n",
      "Вы будете чай ли UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він не піде на пікнік, як і я.\n",
      "\n",
      "Он не пойдет по UNKNOWN как и UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Коли Ви народилися?\n",
      "\n",
      "Когда Вы UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це моя улюблена пісня.\n",
      "\n",
      "Это моя любимая UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ми майже сім’я.\n",
      "\n",
      "мы почти UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Який гарний сьогодні місяць!\n",
      "\n",
      "Какой красивый сегодня UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я проти будь-яких війн.\n",
      "\n",
      "Я против каких-либо UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Поверхня повітряної кулі — неевклідовий простір, тому для неї не виконуються правила евклідової геометрії.\n",
      "\n",
      "поверхность воздушной шары — UNKNOWN UNKNOWN потому для неё не выполняются правила симметрической UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Кажуть, що американці вважають кількість грошей, яку заробляє людина, мірилом його уміння.\n",
      "\n",
      "UNKNOWN что американцы считают количество UNKNOWN какую зарабатывает UNKNOWN мерилом его UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Можна я примірю це плаття?\n",
      "\n",
      "Можно мной UNKNOWN это UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Якщо буде гарна погода, ми доберемося туди завтра.\n",
      "\n",
      "Если будет красивая UNKNOWN мы доберёмся туда UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це був злий заєць.\n",
      "\n",
      "Это был злой UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Один, два, три, чотири, п'ять, шість, сім, вісім, дев'ять, десять.\n",
      "\n",
      "UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Хто в любові не знається, той горя не знає.\n",
      "\n",
      "Кто во любви не UNKNOWN тот горя не UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Його мати хвилюється за нього.\n",
      "\n",
      "Его иметь волнуется за UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я поважаю тих, хто старається з усіх сил.\n",
      "\n",
      "Я уважаю UNKNOWN кто старается со всех UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Їхня дружба переросла у глибоке кохання.\n",
      "\n",
      "необычайная дружба переросла во глубокое UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Кейт п’є багато молока кожен день.\n",
      "\n",
      "Рейчел UNKNOWN много молока каждый UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він злодій.\n",
      "\n",
      "Он UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Шумового забруднення можна було б позбігнути тільки якщо б люди були більш чутливими до навколишнього середовища.\n",
      "\n",
      "UNKNOWN загрязнение можно было бы UNKNOWN только если бы люди были более чувствительны к окружающей UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Чай з лимоном, будьте ласкаві.\n",
      "\n",
      "чай со UNKNOWN будте UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Не плутай бажання з коханням.\n",
      "\n",
      "Не путать желание со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я би з задоволенням написав сотні речень в Tatoeb’і, але в мене є справи.\n",
      "\n",
      "Я бы со удовольствием сочинил сотни сложноподчинённые во UNKNOWN конечно во меня То UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Дайте мені філіжанку кави.\n",
      "\n",
      "Дайте мне чашечку UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Але ж ти ніколи мені про це не розповідала!\n",
      "\n",
      "ведь же ты никогда мне о это не UNKNOWN\n",
      "-------------------------------------------------------\n",
      "У тебе будуть проблеми, якщо твої батьки довідаються.\n",
      "\n",
      "Во тебя будут UNKNOWN если твои родители UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Запах троянд наповнив кімнату.\n",
      "\n",
      "Запах роз наполнил UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Як у тебе справи?\n",
      "\n",
      "Как во тебя UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це мої штани.\n",
      "\n",
      "Это мои UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ні, дякую.\n",
      "\n",
      "UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я не розумію, чому Німеччина перемогла на Євробаченні.\n",
      "\n",
      "Я не UNKNOWN почему Германия победила по UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Добрий вечір.\n",
      "\n",
      "Добрый UNKNOWN\n",
      "-------------------------------------------------------\n",
      "З юбілеєм Олексія Дударева привітав Президент Білорусі Олександр Лукашенко.\n",
      "\n",
      "Со UNKNOWN Алексея Палашка поприветствовал президент Белоруссии Александр UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Чумацький шлях — широкий пояс із далеких зірок, кожна зірка — сонце, таке як наше.\n",
      "\n",
      "Млечный путь — широкий пояс со далеких UNKNOWN каждая звезда — UNKNOWN такое как UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Незвичайно бачити рок-зірок з краваткою!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "удивительно видеть рок-звёзд со UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Усе печиво у формі зірок.\n",
      "\n",
      "всё печенье во форме UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Що мені вдягнути — штани чи спідницю?\n",
      "\n",
      "ЧТо мне одеть — штаны ли UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Гартман Вітвер — відомий львівський скульптор.\n",
      "\n",
      "Краусс утверждал — известный московский UNKNOWN\n",
      "-------------------------------------------------------\n",
      "То був злий кролик.\n",
      "\n",
      "Ой был злой UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Можеш взяти будь-який, що тобі до сподоби.\n",
      "\n",
      "Можешь взять UNKNOWN что тебе к UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Звичайно я піду.\n",
      "\n",
      "Конечно мной UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Шовкопряди прядуть кокони.\n",
      "\n",
      "шелковичные прядут UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Що б ти зробила, якщо б у тебе було, скажім, десять тисяч доларів?\n",
      "\n",
      "ЧТо бы ты UNKNOWN если бы во тебя UNKNOWN UNKNOWN десять тысяч UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він думає, що він хтось, а насправді він ніхто.\n",
      "\n",
      "Он UNKNOWN что он UNKNOWN а действительно он UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Вона дуже пишається своєю колекцією марок.\n",
      "\n",
      "она очень гордится своею коллекцией UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він дуже простий...\n",
      "\n",
      "Он очень UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Яка ти добра!\n",
      "\n",
      "Какая ты UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Як я за тобою скучив!\n",
      "\n",
      "Как мной за тобой UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це все, що я знаю.\n",
      "\n",
      "Это UNKNOWN что мной UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ти ведеш щоденник?\n",
      "\n",
      "Ты ведёшь UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Тобі вирішувати.\n",
      "\n",
      "Тебе UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це пошта, а то — банк.\n",
      "\n",
      "Это UNKNOWN а то — UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Це все, що я хочу зробити.\n",
      "\n",
      "Это UNKNOWN что мной хочу UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я вперше дивлюся такий страшний фільм.\n",
      "\n",
      "Я впервые смотрю такой страшный UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ця пісня нагадує мені про дім.\n",
      "\n",
      "Этa песня напоминает мне о UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Хіросі тут?\n",
      "\n",
      "Хироси UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Мене звуть Джек.\n",
      "\n",
      "Меня зовут UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Як людина живе, так вона і помре.\n",
      "\n",
      "Как женщина UNKNOWN так она и UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я тут уже дві години.\n",
      "\n",
      "Я здесь уже две UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Мені треба вибачитись перед Ен.\n",
      "\n",
      "Мне надо извиниться перед UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Сьогодні я бачив шпака.\n",
      "\n",
      "Сегодня мной видел UNKNOWN\n",
      "-------------------------------------------------------\n",
      "«Скільки коштує ця носова хусточка?» — «Дев'яносто п'ять центів».\n",
      "\n",
      "UNKNOWN стоить та носовая UNKNOWN — UNKNOWN UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Ранені ведмеді, як правило, дуже небезпечні.\n",
      "\n",
      "солдаты UNKNOWN как UNKNOWN очень UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він швидко втомлюється.\n",
      "\n",
      "Он быстро UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Усі готові.\n",
      "\n",
      "остальные UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Він скучає по своїй сім'ї.\n",
      "\n",
      "Он скучает по своей UNKNOWN\n",
      "-------------------------------------------------------\n",
      "«Дякую», — «На здоров'я».\n",
      "\n",
      "UNKNOWN — UNKNOWN UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Я ще не знаю своєї адреси, я певний час буду жити в подруги.\n",
      "\n",
      "Я ещe не знаю своего UNKNOWN мной определенный момент буду жить во UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Амазонка— друга по довжині ріка в світі після Ніла.\n",
      "\n",
      "UNKNOWN вторая по длине река во мире после UNKNOWN\n",
      "-------------------------------------------------------\n",
      "А якщо побачиш Тома, передай йому від мене вітання.\n",
      "\n",
      "А если увидишь UNKNOWN передай ему от меня UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Закрий за собою двері.\n",
      "\n",
      "закрой за собой UNKNOWN\n",
      "-------------------------------------------------------\n",
      "Тримай при собі словник.\n",
      "\n",
      "Держи при себе UNKNOWN\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent in uk_corpus[::10]:\n",
    "    print(sent)\n",
    "    print(translate(sent))\n",
    "    print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-3173ebdff713>:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  sentence_emb = [[np.matmul(word, W)] if word != 'UNKNOWN' else 'UNKNOWN' for word in sentence_emb]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "океан эльзы\n"
     ]
    }
   ],
   "source": [
    "print(translate('океан ельзи'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! \n",
    "See second notebook for the Neural Machine Translation assignment."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
